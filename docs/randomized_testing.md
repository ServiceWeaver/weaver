# Randomized Testing

When you write a **unit test**, you write some code and check that it executes
in the way you expect. For example, to test a `Reversed([]byte) []byte`
function, you might check that

- `Reversed([]byte{})` is `[]byte{}`,
- `Reversed([]byte{1})` is `[]byte{1}`,
- `Reversed([]byte{1, 2})` is `[]byte{2, 1}`, and so on.

Unit tests are thus limited to only test the behaviors and corner cases that you
can think of and have the patience to write down.

With randomized **property-based tests**, on the other hand, you specify a
property of your code that should always hold and then test the property on
millions of randomly generated examples. For example, we can test that the
`Reversed` function is [involutive][]&mdash;i.e. that `Reversed(Reversed(x)) ==
x`&mdash;by writing a [fuzz test][fuzzing] that tests the property on randomly
generated byte slices:

```go
func FuzzReversedIsInvolutive(f *testing.F) {
    f.Fuzz(func(t *testing.T, want []byte) {
        got := Reversed(Reversed(want))
        if !slices.Equal(want, got) {
            t.Fatalf("got %v, want %v", got, want)
        }
    })
}
```

We can test Service Weaver applications in the same way and check that
properties of an application always hold, even when we run random operations
on random inputs in the face of random failures and random interleavings. For
distributed systems, this kind of randomized property-based testing is
especially valuable, as many failure inducing corner cases are extremely
pathological and hard to think of. [Even well studied and heavily scrutinized
protocols tend to have subtle bugs][protocol_bugs].

## Types of Randomized Testing

In this section, we describe three ways to introduce randomized property-based
testing to Service Weaver. We borrow heavily from [*Testing Distributed
Systems*][testing_distributed_systems] by [Andrey Satarin][asatarin].

The various approaches differ in the following ways.

- What properties can be tested?
- What kind of failures are supported?
- How replayable, readable, and minimal are failing executions?
- How concurrently do things run?
- How complex are the APIs?

### Jepsen Testing

With [Jepsen][jepsen]-style testing, we run a Service Weaver application with
components in their own containers. We execute random client requests against
the system, with requests allowed to execute in parallel. We periodically inject
failures into the system. For example, we can

- kill a container,
- partition containers from one another,
- suspend a process for some time,
- skew a container's clock,
- corrupt a disk, and so on.

A user has to specify which operations to run, how to generate inputs to these
operations, which types of failures to inject, and what properties to check.

**Pros.**
The benefit of Jepsen-style testing is that failures are relatively real. We
don't have to simulate a process failing, for example. We can just straight up
fail it. Injected failures will never sufficiently capture the full set of
failures that can happen in the wild, of course, but they are better than some
other approaches which we'll discuss in a moment. Client requests also run in
parallel, which allows for some interleavings that other approaches don't cover.

**Cons.**
The disadvantage of Jepsen-style testing is that failure inducing executions are
hard to understand, replay, and minimize. An execution can be very long,
consisting of thousands of parallel client requests. Replaying and minimizing an
execution is impossible due to the non-determinism introduced by executing
requests and failures in parallel (this is a problem even if the code itself is
deterministic). Running an app across a set of containers can also be slow.

### Deterministic Simulation

With [deterministic simulation][deterministic_simulation], we run a Service
Weaver application all within a single process. We execute random client
requests concurrently, but not in parallel. Specifically, a client request
executes on its own, but when the client request calls a component method, its
execution is suspended and control is passed to a centralized scheduler. The
scheduler can then

- deliver a pending method call,
- deliver a pending method return,
- deliver an error to a pending method call, or
- start running a new client request.

In this way, execution runs step-by-step, with the simulator deciding exactly
which step to run next. Moreover, assuming the application being tested is
deterministic, a simulator will repeatedly produce the exact same execution when
given the same seed.

Note that failures are implicit in the execution model. To simulate a network
partition, messages between the two sides of the partition are not delivered,
and a replica failure is simulated as a single-node partition.

As with Jepsen-style tests, a user has to specify which operations to run, how
to generate inputs to these operations, which types of failures to inject, and
what properties to check. You can find a simple prototype of a deterministic
simulator [here][sim_demo].

**Pros.**
The benefit of deterministic simulation is that executions are easy to
understand, replay, and minimize. Once a bug is found and fixed, failure
inducing executions can be replayed to check that the bug fix works. Executions
can also be [minimized][minimization], making it much easier to understand the
root cause of the failure.

Deterministic simulation can also enumerate *all* possible message
interleavings, including those that would very rarely occur naturally in a
Jepsen-style test.

Deterministic simulation is also quite fast.

**Cons.**
The disadvantage of deterministic simulation is that, unlike Jepsen-style tests,
it cannot uncover bugs that arise from parallel execution or realistic failures.
For example, consider a bug that only manifests when a component crashes at a
very particular point in the middle of its execution. Because deterministic
simulation only suspends a component at method call boundaries, it won't uncover
this bug.

If the code in a Service Weaver application is not deterministic, then
deterministic simulation is less effective. A failing execution may not always
fail, and minimization may act erratically.

Implementing deterministic simulation is also more complex than implementing
Jepsen-style tests, as the simulator has to cleverly inject itself into the code
at every method call.

### Chaos Testing

[Chaos testing][chaos_monkey] a Service Weaver app is similar to Jepsen testing
it. We run the application in a set of containers, run a random workload against
it, and fail things periodically. The difference is that Jepsen-style testing
tests properties that should always hold, no matter how many failures there are
or how severe the failures are. Chaos testing, on the other hand, tends to test
that certain metrics don't degrade too much in the presence of a more restricted
type of failure. For example, we could test that latency doesn't dip too much in
the presence of any one replica failing.

Chaos testing is supposed to be done in production, but it can also be done
locally with simulated workloads.

**Pros and Cons.**
The pro and con of chaos testing is that we cannot test complex properties
(e.g., linearizability) or complex failures. This makes chaos testing easier to
reason about but also less expressive.

### Proposal

I propose we introduce deterministic simulation style testing to weavertest.
Later, we may be able to implement Jepsen-style testing as well using the same
or very similar API. Both deterministic testing and Jepsen testing require
roughly the same things from the user (which operations to run, how to generate
arguments to these operations, which failures to inject, which properties to
check).

## API

In this section, we propose three options for a deterministic simulation API.
Consider an application with components `A`, and `B`:

```go
// Component A
type A interface { A(context.Context, int) (int, error) }
type a struct{ weaver.Implements[A] }
func (*a) A(context.Context, int) (int, error) { ... }

// Component B
type B interface { B(context.Context) error }
type b struct{ weaver.Implements[B] }
func (*b) B(context.Context) error { ... }

// Component B fake.
type fakeb struct{}
func (*fakeb) B(context.Context) error { ... }
```

We want to simulate this application against a workload with the following
characteristics:

- We have two operations, or ops, to run. One calls `A.A` on randomly generated
  integers, and the other calls `B.B`.
- We want to fake component `B`.
- We have some state that we update as the test runs. Specifically, we
  want to record the set of all successful calls to `A.A`.

### Option 1

Option 1 is shown below. We call `sim.RegisterState[T]` to register a function
that returns a freshly initialized piece of state of type `T`. We call
`sim.RegisterFake[T]` to register a fake for component `T`. We call
`sim.RegisterOp[T]` to register an operation. Every operation has

- a name,
- a way to randomly generate a value of type `T`, and
- the body of the operation, which receives the state, the randomly
  generated value of type `T`, and any components it needs to execute.

Finally, we run the simulator. It's important to note that `s.Run()` runs many
simulations, potentially on many threads. This is why `RegisterState` and
`RegisterFake` take functions as arguments. The simulator has to create a new
state and new fake for every simulation.

```go
func TestWorkload(t *testing.T) {
    // Create the simulator.
    s, err := sim.New(sim.Options{...})
    if err != nil {
        t.Fatal(err)
    }

    // Register state of type map[int]int.
    sim.RegisterState(s, func() map[int]int {
        return map[int]int{}
    })

    // Register a fake for component B.
    sim.RegisterFake[B](s, func() B {
        return &fakeb{}
    })

    // Register an op to call A.A.
    sim.RegisterOp(s, sim.Op[int]{
        Name: "CallA",
        Gen:  func(*rand.Rand) int { ... },
        Func: func(ctx context.Context, replies map[int]int, x int, a A) error {
            if y, err := a.A(ctx, x); err == nil {
                replies[x] = y
            }
            return nil
        },
    })

    // Register an op to call B.B.
    sim.RegisterOp(s, sim.Op[struct{}]{
        Name: "CallB",
        Gen:  func(*rand.Rand) struct{} { ... },
        Func: func(ctx context.Context, _ map[int]int, _ struct{}, b B) error {
            b.B(ctx)
            return nil
        },
    })

    // Run the simulator for 10 seconds.
    results, err := s.Run(10*time.Second)
    if err != nil {
        t.Fatal(err)
    }
}
```

**Pros.**
This API is very direct. It leverages the Go type system when possible
(`RegisterState[T]`, `RegisterFake[T]`, `Op[T]`), but does lean on reflection in
places.

**Cons.**
This API is unergonomic. Threading state through every single op, even those
that don't need it, is cumbersome. Registering state and fakes through generator
functions is also a bit awkward. Fakes don't have a way to read or write the
state, which can be very useful for some tests.

### Option 2

Option 2 is shown below. `s.Run` mirrors `T.Run` from the `testing` package. We
call `s.Run` with a lambda that performs a single simulation. Inside the lambda,
state is represented as local variables. Fakes are registered as values,
rather than constructor functions. Ops are the same as in Option 1, except that
they don't need to take state as an argument.

```go
func TestWorkload(t *testing.T) {
    // Create the simulator.
    s, err := sim.New(sim.Options{...})
    if err != nil {
        t.Fatal(err)
    }

    // Run the simulator for 10 seconds.
    results, err := s.Run(10*time.Second, func(s *sim.Sim) error {
        // State is a local variable.
        replies := map[int]int{}

        // Register a fake value.
        sim.RegisterFake[B](s, &fakeb{})

        // Register an op to call A.A.
        sim.RegisterOp(s, sim.Op[int]{
            Name: "CallA",
            Gen:  func(*rand.Rand) int { ... },
            Func: func(ctx context.Context, x int, a A) error {
                if y, err := a.A(ctx, x); err == nil {
                    replies[x] = y
                }
                return nil
            },
        })

        // Register an op to call B.B.
        sim.RegisterOp(s, sim.Op[struct{}]{
            Name: "CallB",
            Gen:  func(*rand.Rand) struct{} { ... },
            Func: func(ctx context.Context, _ struct{}, b B) error {
                b.B(ctx)
                return nil
            },
        })

        // Run a single simulation.
        return s.Run()
    })
    if err != nil {
        t.Fatal(err)
    }
}
```

**Pros.**
The main benefit of this approach is that state is represented as local
variables, and fakes are registered by value.

**Cons.**
This API has two distinct simulator types: one to run a bunch of simulations and
one to run a single simulation. The API cannot prevent someone from registering
different state, fakes, and ops for every individual simulation, which is not
intended. More generally, it's a bit odd that you keep re-registering everything
for every individual simulation.

### Option 3

Option 3 is shown below. This option wraps all the logic of the workload into
the `Workload` struct, which is passed to the `sim.New[T]` function. The
`Workload` struct has `weaver.Ref`s to the components needed in the test. It
also has fields for any state.

- Method `FakeB` returns a fake for component `B`. Generally, method `FakeFoo`
  returns a fake for component `Foo`.
- Method `GenCallA` generates the random values passed to `CallA`. Generally,
  `GenFoo` generates the random values passed to `Foo`.
- Methods `CallA` and `CallB` are the body of the ops. Generally, all exported
  methods that don't begin with `Fake` or `Gen` are ops.

```go
type Workload struct {
    a weaver.Ref[A]
    b weaver.Ref[B]
    replies map[int]int
}

func (w *Workload) Init(context.Context) error {
    w.replies = map[int]int{}
    return nil
}

func (w *Workload) FakeB(context.Context) B {
    return &fakeb{}
}

func (w *Workload) CallA(ctx context.Context, x int) error {
    if y, err := w.a.Get().A(ctx, x); err == nil {
        w.replies[x] = y
    }
    return nil
}

func (*Workload) GenCallA(r *rand.Rand) int {
    ...
}

func (w *Workload) CallB(ctx context.Context) error {
    w.b.Get().B(ctx)
    return nil
}

func TestWorkload(t *testing.T) {
    s, err := sim.New[Workload](sim.Options{...})
    if err != nil {
        t.Fatal(err)
    }
    results, err := s.Simulate(10 * time.Second)
    if err != nil {
        t.Fatal(err)
    }
}
```

**Pros.**
This API is arguably more ergonomic than the other options.

**Cons.**
This option is very magical. Rather than calling library functions like
`sim.RegisterOp`, for example, you have to know the special syntax to create an
op.

### Decision

NOTE(mwhittaker): I don't love any of these options. I have a slight preference
for Option 3, but am very open to other ideas or ways to improve an existing
option.

[asatarin]: https://twitter.com/asatarin
[chaos_monkey]: https://netflix.github.io/chaosmonkey/
[deterministic_simulation]: https://asatarin.github.io/testing-distributed-systems/#deterministic-simulation
[fuzzing]: https://go.dev/doc/tutorial/fuzz
[involutive]: https://en.wikipedia.org/wiki/Involution_(mathematics)
[jepsen]: https://jepsen.io/
[minimization]: https://www.usenix.org/system/files/conference/nsdi16/nsdi16-paper-scott.pdf
[protocol_bugs]: https://github.com/dranov/protocol-bugs-list
[sim_demo]: https://github.com/ServiceWeaver/weaver/tree/sim/internal/sim
[testing_distributed_systems]: https://asatarin.github.io/testing-distributed-systems/
