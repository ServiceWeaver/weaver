// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package babysitter

import (
	"context"
	"crypto/sha256"
	_ "embed"
	"errors"
	"fmt"
	"math"
	"net"
	"net/http"
	"os"
	"sort"
	"sync"
	"syscall"

	"github.com/ServiceWeaver/weaver/internal/envelope/conn"
	"github.com/ServiceWeaver/weaver/internal/logtype"
	imetrics "github.com/ServiceWeaver/weaver/internal/metrics"
	"github.com/ServiceWeaver/weaver/internal/net/call"
	"github.com/ServiceWeaver/weaver/internal/proxy"
	"github.com/ServiceWeaver/weaver/internal/status"
	"github.com/ServiceWeaver/weaver/internal/versioned"
	"github.com/ServiceWeaver/weaver/runtime/envelope"
	"github.com/ServiceWeaver/weaver/runtime/logging"
	"github.com/ServiceWeaver/weaver/runtime/metrics"
	"github.com/ServiceWeaver/weaver/runtime/perfetto"
	"github.com/ServiceWeaver/weaver/runtime/protomsg"
	"github.com/ServiceWeaver/weaver/runtime/protos"
	"github.com/ServiceWeaver/weaver/runtime/retry"
	"github.com/google/uuid"
	"go.opentelemetry.io/otel/sdk/trace"
	"google.golang.org/protobuf/proto"
	"google.golang.org/protobuf/types/known/timestamppb"
)

const (
	// The default replication factor for a component.
	DefaultReplication = 2

	// routingInfoKey is the key where we track routing information for a given process.
	routingInfoKey = "routing_entries"

	// appVersionStateKey is the key where we track the state for a given application version.
	appVersionStateKey = "app_version_state"
)

// See createAndRunEnvelopeForMain.
var dontGC []*os.File

// Babysitter manages an application version deployment.
type Babysitter struct {
	ctx    context.Context
	opts   envelope.Options
	dep    *protos.Deployment
	logger logtype.Logger

	// logSaver processes log entries generated by the weavelet. The entries
	// either have the timestamp produced by the weavelet, or have a nil Time
	// field. Defaults to a log saver that pretty prints log entries to stderr.
	//
	// logSaver is called concurrently from multiple goroutines, so it should
	// be thread safe.
	logSaver func(*protos.LogEntry)

	// traceSaver processes trace spans generated by the weavelet. If nil,
	// weavelet traces are dropped.
	//
	// traceSaver is called concurrently from multiple goroutines, so it should
	// be thread safe.
	traceSaver func([]trace.ReadOnlySpan) error

	// statsProcessor tracks and computes stats to be rendered on the /statusz page.
	statsProcessor *imetrics.StatsProcessor

	mu          sync.RWMutex
	managed     map[string][]*envelope.Envelope
	appInfo     *versioned.Map[*AppVersionState]
	routingInfo *versioned.Map[*protos.RoutingInfo]
	proxies     map[string]*proxyInfo // proxies, by listener name
}

type proxyInfo struct {
	proxy *proxy.Proxy
	addr  string // dialable address of the proxy
}

var _ envelope.EnvelopeHandler = &Babysitter{}

// NewBabysitter creates a new babysitter.
func NewBabysitter(ctx context.Context, dep *protos.Deployment, logSaver func(*protos.LogEntry)) (*Babysitter, error) {
	logger := logging.FuncLogger{
		Opts: logging.Options{
			App:       dep.App.Name,
			Component: "babysitter",
			Weavelet:  uuid.NewString(),
			Attrs:     []string{"serviceweaver/system", ""},
		},
		Write: logSaver,
	}

	// Create the trace saver.
	traceDB, err := perfetto.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("cannot open Perfetto database: %w", err)
	}
	traceSaver := func(spans []trace.ReadOnlySpan) error {
		return traceDB.Store(ctx, dep.App.Name, dep.Id, spans)
	}

	b := &Babysitter{
		ctx:            ctx,
		logger:         logger,
		logSaver:       logSaver,
		traceSaver:     traceSaver,
		statsProcessor: imetrics.NewStatsProcessor(),
		opts:           envelope.Options{Restart: envelope.Never, Retry: retry.DefaultOptions},
		dep:            dep,
		managed:        map[string][]*envelope.Envelope{},
		appInfo:        versioned.NewMap[*AppVersionState](),
		routingInfo:    versioned.NewMap[*protos.RoutingInfo](),
		proxies:        map[string]*proxyInfo{},
	}
	go b.statsProcessor.CollectMetrics(b.ctx, b.readMetrics)
	return b, nil
}

// RegisterStatusPages registers the status pages with the provided mux.
func (b *Babysitter) RegisterStatusPages(mux *http.ServeMux) {
	status.RegisterServer(mux, b, b.logger)
}

// StartColocationGroup implements the protos.EnvelopeHandler interface.
func (b *Babysitter) StartColocationGroup(req *protos.ColocationGroup) error {
	b.mu.Lock()
	defer b.mu.Unlock()

	if _, ok := b.managed[req.Name]; ok {
		return nil
	}
	b.managed[req.Name] = []*envelope.Envelope{}

	// Manage the colocation group.
	go b.manage(b.dep, req)
	return nil
}

// StartComponent implements the protos.EnvelopeHandler interface.
func (b *Babysitter) StartComponent(req *protos.ComponentToStart) error {
	b.mu.Lock()
	defer b.mu.Unlock()

	// Note that the babysitter handles a single application deployment.
	existing, _, err := b.appInfo.Read(b.ctx, appVersionStateKey, "")
	if err != nil {
		return err
	}
	if existing == nil {
		existing = &AppVersionState{
			App:            req.App,
			DeploymentId:   req.DeploymentId,
			SubmissionTime: timestamppb.Now(),
			Groups:         map[string]*ColocationGroupState{},
			Processes:      map[string]*ProcessState{},
		}
	}
	if _, found := existing.Groups[req.ColocationGroup]; !found {
		existing.Groups[req.ColocationGroup] = &ColocationGroupState{}
	}
	group := existing.Groups[req.ColocationGroup]
	found := false
	for _, groupProc := range group.Processes {
		if req.Process == groupProc {
			found = true
			break
		}
	}
	if !found {
		group.Processes = append(group.Processes, req.Process)
	}

	proc := findOrCreateProcess(req.App, req.DeploymentId, req.Process, existing)
	if _, found := proc.Components[req.Component]; !found {
		proc.Components[req.Component] = req.IsRouted
	}
	if req.IsRouted {
		if _, found := proc.Assignments[req.Component]; !found {
			// Create an initial assignment for the component.
			proc.Assignments[req.Component] = &protos.Assignment{
				App:          req.App,
				DeploymentId: req.DeploymentId,
				Component:    req.Component,
			}
		}
	}
	if err := b.mayGenerateNewRoutingInfo(existing, req.App, req.DeploymentId, req.Process); err != nil {
		return err
	}
	b.appInfo.Update(appVersionStateKey, existing)
	return nil
}

// RegisterReplica implements the protos.EnvelopeHandler interface.
func (b *Babysitter) RegisterReplica(req *protos.ReplicaToRegister) error {
	b.mu.Lock()
	defer b.mu.Unlock()

	existing, _, err := b.appInfo.Read(b.ctx, appVersionStateKey, "")
	if err != nil {
		return err
	}
	proc := findOrCreateProcess(req.App, req.DeploymentId, req.Process, existing)

	var found bool
	for _, replica := range proc.Replicas {
		if req.Address == replica {
			// If the replica is already registered, then we skip any writes.
			found = true
			break
		}
	}
	if !found {
		proc.Replicas = append(proc.Replicas, req.Address)
		proc.ReplicaPids = append(proc.ReplicaPids, req.Pid)
	}

	if err := b.mayGenerateNewRoutingInfo(existing, req.App, req.DeploymentId, req.Process); err != nil {
		return err
	}
	b.appInfo.Update(appVersionStateKey, existing)
	return nil
}

// GetRoutingInfo implements the protos.EnvelopeHandler interface.
func (b *Babysitter) GetRoutingInfo(req *protos.GetRoutingInfo) (*protos.RoutingInfo, error) {
	existing, newVersion, err := b.routingInfo.Read(b.ctx, toKey(req.Process), req.Version)
	if err != nil {
		return nil, err
	}
	if existing == nil {
		existing = &protos.RoutingInfo{}
	}
	existing.Version = newVersion
	return existing, nil
}

// GetComponentsToStart implements the protos.EnvelopeHandler interface.
func (b *Babysitter) GetComponentsToStart(req *protos.GetComponentsToStart) (*protos.ComponentsToStart, error) {
	// Fetch components to start.
	existing, newVersion, err := b.appInfo.Read(b.ctx, appVersionStateKey, req.Version)
	if err != nil {
		return nil, err
	}

	var reply protos.ComponentsToStart
	reply.Version = newVersion

	if existing != nil {
		if ps, found := existing.Processes[req.Process]; found {
			for c := range ps.Components {
				reply.Components = append(reply.Components, c)
			}
		}
	}
	return &reply, nil
}

// RecvLogEntry implements the protos.EnvelopeHandler interface.
func (b *Babysitter) RecvLogEntry(entry *protos.LogEntry) {
	b.logSaver(entry)
}

// RecvTraceSpans implements the protos.EnvelopeHandler interface.
func (b *Babysitter) RecvTraceSpans(spans []trace.ReadOnlySpan) error {
	if b.traceSaver == nil {
		return nil
	}
	return b.traceSaver(spans)
}

// ReportLoad implements the protos.EnvelopeHandler interface.
func (b *Babysitter) ReportLoad(*protos.WeaveletLoadReport) error {
	return nil
}

// ExportListener implements the protos.EnvelopeHandler interface.
func (b *Babysitter) ExportListener(req *protos.ListenerToExport) (*protos.ExportListenerReply, error) {
	b.mu.Lock()
	defer b.mu.Unlock()

	existing, _, err := b.appInfo.Read(b.ctx, appVersionStateKey, "")
	if err != nil {
		return nil, err
	}
	if existing == nil {
		existing = &AppVersionState{
			App:            b.dep.App.Name,
			DeploymentId:   b.dep.Id,
			SubmissionTime: timestamppb.Now(),
			Groups:         map[string]*ColocationGroupState{},
			Processes:      map[string]*ProcessState{},
		}
	}
	existing.Listeners = append(existing.Listeners, req.Listener)
	b.appInfo.Update(appVersionStateKey, existing)

	if p, ok := b.proxies[req.Listener.Name]; ok {
		p.proxy.AddBackend(req.Listener.Addr)
		return &protos.ExportListenerReply{ProxyAddress: p.addr}, nil
	}

	lis, err := net.Listen("tcp", req.LocalAddress)
	if errors.Is(err, syscall.EADDRINUSE) {
		return &protos.ExportListenerReply{AlreadyInUse: true}, nil
	}
	if err != nil {
		return nil, fmt.Errorf("proxy listen: %w", err)
	}
	addr := lis.Addr().String()
	b.logger.Info("Proxy listening", "address", addr)
	proxy := proxy.NewProxy(b.logger)
	proxy.AddBackend(req.Listener.Addr)
	b.proxies[req.Listener.Name] = &proxyInfo{proxy: proxy, addr: addr}
	go func() {
		if err := serveHTTP(b.ctx, lis, proxy); err != nil {
			b.logger.Error("proxy", err)
		}
	}()
	return &protos.ExportListenerReply{ProxyAddress: addr}, nil
}

// manage runs the Envelope management loop for a given deployment group.
func (b *Babysitter) manage(dep *protos.Deployment, group *protos.ColocationGroup) error {
	var version *call.Version
	for r := retry.Begin(); r.Continue(b.ctx); {
		procsToStart, newVersion, err := b.getProcessesToStart(group.Name, version)
		if err != nil {
			continue
		}
		version = newVersion
		for _, proc := range procsToStart {
			if err := b.startProcess(dep, group, proc); err != nil {
				b.logger.Error("Error starting process", err, "process", proc)
			}
		}
		r.Reset()
	}
	return b.ctx.Err()
}

func (b *Babysitter) startProcess(dep *protos.Deployment, group *protos.ColocationGroup, proc string) error {
	b.mu.Lock()
	defer b.mu.Unlock()

	envelopes, ok := b.managed[proc]
	if ok && len(envelopes) == DefaultReplication {
		// Already started.
		return nil
	}

	for r := 0; r < DefaultReplication; r++ {
		// Note that we assign a unique UUID for each group replica. This is because
		// we use the group replica ids to create replica-local addresses to
		// communicate between the weavelets.
		id := uuid.NewHash(sha256.New(), uuid.Nil, []byte(fmt.Sprintf("%d", r)), 0).String()

		// Start the weavelet and capture its logs, traces, and metrics.
		wlet := &protos.WeaveletInfo{
			App:               dep.App.Name,
			DeploymentId:      dep.Id,
			Group:             group,
			GroupId:           id,
			Process:           proc,
			Id:                uuid.New().String(),
			SameProcess:       dep.App.SameProcess,
			Sections:          dep.App.Sections,
			SingleProcess:     dep.SingleProcess,
			UseLocalhost:      dep.UseLocalhost,
			ProcessPicksPorts: dep.ProcessPicksPorts,
			NetworkStorageDir: dep.NetworkStorageDir,
		}
		e, err := envelope.NewEnvelope(wlet, dep.App, b, b.opts)
		if err != nil {
			return err
		}
		go func() {
			// TODO(mwhittaker): Propagate errors.
			if err := e.Run(b.ctx); err != nil {
				b.logger.Error("e.Run", err)
			}
		}()
		b.managed[proc] = append(b.managed[proc], e)
	}
	return nil
}

func (b *Babysitter) getProcessesToStart(group string, version *call.Version) (
	[]string, *call.Version, error) {
	// Fetch processes to start.
	var v string
	if version != nil {
		v = version.Opaque
	}

	existing, newVersion, err := b.appInfo.Read(b.ctx, appVersionStateKey, v)
	if err != nil {
		return nil, nil, err
	}

	var processes []string
	if existing != nil {
		if gs, found := existing.Groups[group]; found {
			processes = append(processes, gs.Processes...)
		}
	}
	return processes, &call.Version{Opaque: newVersion}, nil
}

// CreateAndRunEnvelopeForMain creates an envelope for an already started main.
// It is used by weavertest where the main process is already running.
func (b *Babysitter) CreateAndRunEnvelopeForMain(mainWlet *protos.WeaveletInfo, config *protos.AppConfig) (int, int, error) {
	// Set up the pipes between the envelope and the main weavelet. The
	// pipes will be closed by the envelope and weavelet conns.
	//
	//         envelope                      weavelet
	//         --------        +----+        --------
	//   fromWeaveletReader <--| OS |<-- fromWeaveletWriter
	//     toWeaveletWriter -->|    |--> toWeaveletReader
	//                         +----+
	fromWeaveletReader, fromWeaveletWriter, err := os.Pipe()
	if err != nil {
		return 0, 0, fmt.Errorf("cannot create fromWeavelet pipe: %v", err)
	}
	toWeaveletReader, toWeaveletWriter, err := os.Pipe()
	if err != nil {
		return 0, 0, fmt.Errorf("cannot create toWeavelet pipe: %v", err)
	}

	// There is a very subtle bug we have to avoid. As explained in the
	// official godoc [1], if an *os.File is garbage collected, the underlying
	// file is closed. Thus, if toWeaveletReader or fromWeaveletWriter are
	// garbage collected, the underlying pipes are closed.
	//
	// The main weavelet, which is running in a separate goroutine, is reading
	// from and writing to these pipes, but it doesn't use toWeaveletReader or
	// fromWeaveletWriter directly. Instead, it constructs new files using the
	// two pipe's file descriptors. This means that Go is free to garbage
	// collect fromWeaveletWriter and toWeaveletWriter even though the
	// underlying pipes are still being used by the main weavelet.
	//
	// If the pipes get closed while the main weavelet is running, then the
	// weavelet can crash with all sorts of errors (e.g., bad pipe, resource
	// temporarily unavailable, nil pointer dereferences, etc). You can run [2]
	// locally to reproduce this behavior. If the program doesn't crash on your
	// machine, try increasing the number of iterations.
	//
	// To avoid this, we have to ensure that the pipes are not garbage
	// collected prematurely. For now, we place them in a global variable. It's
	// not a great solution, but it gets the job done.
	//
	// [1]: https://pkg.go.dev/os#File.Fd
	// [2]: https://go.dev/play/p/9JG7voL2oHP
	//
	// TODO(mwhittaker): Think of a less janky solution to this bug.
	b.mu.Lock()
	dontGC = append(dontGC, fromWeaveletReader, fromWeaveletWriter,
		toWeaveletReader, toWeaveletWriter)
	b.mu.Unlock()

	// Create a connection between the weavelet for the main process and the envelope.
	econn, err := conn.NewEnvelopeConn(fromWeaveletReader, toWeaveletWriter, b, mainWlet)
	if err != nil {
		return 0, 0, fmt.Errorf("cannot create envelope conn: %v", err)
	}
	go econn.Run()

	// Create and run an envelope for the main process.
	options := envelope.Options{
		Restart:         envelope.Never,
		Retry:           retry.DefaultOptions,
		GetEnvelopeConn: func() *conn.EnvelopeConn { return econn },
	}
	e, err := envelope.NewEnvelope(mainWlet, config, b, options)
	if err != nil {
		return 0, 0, fmt.Errorf("cannot create envelope: %v", err)
	}
	go e.Run(b.ctx)

	return int(toWeaveletReader.Fd()), int(fromWeaveletWriter.Fd()), nil
}

func (b *Babysitter) getEnvelopes() []*envelope.Envelope {
	b.mu.RLock()
	defer b.mu.RUnlock()

	var envelopes []*envelope.Envelope
	for _, envs := range b.managed {
		envelopes = append(envelopes, envs...)
	}
	return envelopes
}

func (b *Babysitter) getManagedProcesses() map[string][]*envelope.Envelope {
	b.mu.RLock()
	defer b.mu.RUnlock()
	res := map[string][]*envelope.Envelope{}
	for proc, envs := range b.managed {
		res[proc] = append(res[proc], envs...)
	}
	return res
}

func (b *Babysitter) readMetrics() []*metrics.MetricSnapshot {
	var ms []*metrics.MetricSnapshot
	for _, e := range b.getEnvelopes() {
		m, err := e.ReadMetrics()
		if err != nil {
			continue
		}
		ms = append(ms, m...)
	}
	return append(ms, metrics.Snapshot()...)
}

// Profile implements the status.Server interface.
func (b *Babysitter) Profile(_ context.Context, req *protos.RunProfiling) (*protos.Profile, error) {
	profile, err := runProfiling(b.ctx, req, b.getManagedProcesses())
	if err != nil {
		return nil, err
	}
	profile.AppName = b.dep.App.Name
	profile.VersionId = b.dep.Id
	return profile, nil
}

// Status implements the status.Server interface.
func (b *Babysitter) Status(ctx context.Context) (*status.Status, error) {
	info, _, err := b.appInfo.Read(ctx, appVersionStateKey, "")
	if err != nil {
		return nil, err
	}

	stats := b.statsProcessor.GetStatsStatusz()
	var components []*status.Component
	for procName, proc := range info.Processes {
		for component := range proc.Components {
			c := &status.Component{
				Name:    component,
				Process: procName,
				Pids:    proc.ReplicaPids,
			}
			components = append(components, c)

			// TODO(mwhittaker): Unify with ui package and remove duplication.
			s := stats[logging.ShortenComponent(component)]
			if s == nil {
				continue
			}
			for _, methodStats := range s {
				c.Methods = append(c.Methods, &status.Method{
					Name: methodStats.Name,
					Minute: &status.MethodStats{
						NumCalls:     methodStats.Minute.NumCalls,
						AvgLatencyMs: methodStats.Minute.AvgLatencyMs,
						RecvKbPerSec: methodStats.Minute.RecvKBPerSec,
						SentKbPerSec: methodStats.Minute.SentKBPerSec,
					},
					Hour: &status.MethodStats{
						NumCalls:     methodStats.Hour.NumCalls,
						AvgLatencyMs: methodStats.Hour.AvgLatencyMs,
						RecvKbPerSec: methodStats.Hour.RecvKBPerSec,
						SentKbPerSec: methodStats.Hour.SentKBPerSec,
					},
					Total: &status.MethodStats{
						NumCalls:     methodStats.Total.NumCalls,
						AvgLatencyMs: methodStats.Total.AvgLatencyMs,
						RecvKbPerSec: methodStats.Total.RecvKBPerSec,
						SentKbPerSec: methodStats.Total.SentKBPerSec,
					},
				})
			}
		}
	}

	b.mu.Lock()
	defer b.mu.Unlock()
	var listeners []*status.Listener
	for name, proxy := range b.proxies {
		listeners = append(listeners, &status.Listener{
			Name: name,
			Addr: proxy.addr,
		})
	}

	return &status.Status{
		App:            info.App,
		DeploymentId:   info.DeploymentId,
		SubmissionTime: info.SubmissionTime,
		Components:     components,
		Listeners:      listeners,
		Config:         b.dep.App,
	}, nil
}

// Metrics implements the status.Server interface.
func (b *Babysitter) Metrics(ctx context.Context) (*status.Metrics, error) {
	m := &status.Metrics{}
	for _, snap := range b.readMetrics() {
		m.Metrics = append(m.Metrics, snap.ToProto())
	}
	return m, nil
}

// mayGenerateNewRoutingInfo may generate new routing information for a given process.
//
// New routing information is generated when (1) new sharded components are managed
// by the process and/or when (2) the process has new replicas.
//
// REQUIRES: b.mu is held.
func (b *Babysitter) mayGenerateNewRoutingInfo(existing *AppVersionState, app, id, process string) error {
	// May compute new assignments.
	proc := findOrCreateProcess(app, id, process, existing)

	for component, currAssignment := range proc.Assignments {
		newAssignment, err := routingAlgo(currAssignment, proc.Replicas)
		if err != nil || newAssignment == nil {
			continue // don't update assignments
		}
		proc.Assignments[component] = newAssignment
	}

	// Update the routing information.
	sort.Strings(proc.Replicas)
	routingInfo := protos.RoutingInfo{
		Replicas: proc.Replicas,
	}
	for _, assignment := range proc.Assignments {
		routingInfo.Assignments = append(routingInfo.Assignments, assignment)
	}
	return b.updateRoutingInfo(process, &routingInfo)
}

// updateRoutingInfo update the state with the latest routing info for a process.
//
// REQUIRES: b.mu is held.
func (b *Babysitter) updateRoutingInfo(process string, routingInfo *protos.RoutingInfo) error {
	key := toKey(process)

	var existing *protos.RoutingInfo
	existing, _, err := b.routingInfo.Read(b.ctx, key, "")
	if err != nil {
		return err
	}
	if existing == nil {
		existing = &protos.RoutingInfo{}
	}
	if !proto.Equal(existing, routingInfo) {
		existing.Reset()
		proto.Merge(existing, routingInfo)
	}

	b.routingInfo.Update(key, existing)
	return nil
}

// routingAlgo is an implementation of a routing algorithm that distributes the
// entire key space approximately equally across all healthy resources.
//
// The algorithm is as follows:
// - split the entire key space in a number of slices that is more likely to
// spread uniformly the key space among all healthy resources
//
// - distribute the slices round robin across all healthy resources
func routingAlgo(currAssignment *protos.Assignment, candidates []string) (*protos.Assignment, error) {
	newAssignment := protomsg.Clone(currAssignment)
	newAssignment.Version++

	// Note that the healthy resources should be sorted. This is required because
	// we want to do a deterministic assignment of slices to resources among
	// different invocations, to avoid unnecessary churn while generating
	// new assignments.
	sort.Strings(candidates)

	if len(candidates) == 0 {
		newAssignment.Slices = nil
		return newAssignment, nil
	}

	const minSliceKey = 0
	const maxSliceKey = math.MaxUint64

	// If there is only one healthy resource, assign the entire key space to it.
	if len(candidates) == 1 {
		newAssignment.Slices = []*protos.Assignment_Slice{
			{Start: minSliceKey, Replicas: candidates},
		}
		return newAssignment, nil
	}

	// Compute the total number of slices in the assignment.
	numSlices := nextPowerOfTwo(len(candidates))

	// Split slices in equal subslices in order to generate numSlices.
	splits := [][]uint64{{minSliceKey, maxSliceKey}}
	var curr []uint64
	for ok := true; ok; ok = len(splits) != numSlices {
		curr, splits = splits[0], splits[1:]
		midPoint := curr[0] + uint64(math.Floor(0.5*float64(curr[1]-curr[0])))
		splitl := []uint64{curr[0], midPoint}
		splitr := []uint64{midPoint, curr[1]}
		splits = append(splits, splitl, splitr)
	}

	// Sort the computed slices in increasing order based on the start key, in
	// order to provide a deterministic assignment across multiple runs, hence to
	// minimize churn.
	sort.Slice(splits, func(i, j int) bool {
		return splits[i][0] <= splits[j][0]
	})

	// Assign the computed slices to resources in a round robin fashion.
	slices := make([]*protos.Assignment_Slice, len(splits))
	rId := 0
	for i, s := range splits {
		slices[i] = &protos.Assignment_Slice{
			Start:    s[0],
			Replicas: []string{candidates[rId]},
		}
		rId = (rId + 1) % len(candidates)
	}
	newAssignment.Slices = slices
	return newAssignment, nil
}

// nextPowerOfTwo returns the next power of 2 that is greater or equal to x.
func nextPowerOfTwo(x int) int {
	// If x is already power of 2, return x.
	if x&(x-1) == 0 {
		return x
	}
	return int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))
}

func findOrCreateProcess(app string, id string, process string, ap *AppVersionState) *ProcessState {
	if ap == nil {
		ap = &AppVersionState{
			App:            app,
			DeploymentId:   id,
			SubmissionTime: timestamppb.Now(),
			Groups:         map[string]*ColocationGroupState{},
			Processes:      map[string]*ProcessState{},
		}
	}
	if ap.Processes == nil {
		ap.Processes = map[string]*ProcessState{}
	}
	if _, found := ap.Processes[process]; !found {
		ap.Processes[process] = &ProcessState{
			Components:  map[string]bool{},
			Assignments: map[string]*protos.Assignment{},
		}
	}
	proc := ap.Processes[process]
	if proc.Components == nil {
		proc.Components = map[string]bool{}
	}
	if proc.Assignments == nil {
		proc.Assignments = map[string]*protos.Assignment{}
	}
	return proc
}

func toKey(process string) string {
	return routingInfoKey + "/" + process
}

// serveHTTP serves HTTP traffic on the provided listener using the provided
// handler. The server is shut down when then provided context is cancelled.
func serveHTTP(ctx context.Context, lis net.Listener, handler http.Handler) error {
	server := http.Server{Handler: handler}
	errs := make(chan error, 1)
	go func() { errs <- server.Serve(lis) }()
	select {
	case err := <-errs:
		return err
	case <-ctx.Done():
		return server.Shutdown(ctx)
	}
}
